training:
  dataset_name: librispeech100h
  output_dir: checkpoints/MelCausalVAE/exp-efficient # variable rate training
  num_train_epochs: 10
  learning_rate: 1e-4
  per_device_train_batch_size: 32
  report_to: wandb
  wandb_project: flexi-vae
  wandb_run_name: exp-efficient-aligner
  fp16: false
  bf16: true
  save_steps: 1000
  eval_strategy: "steps"
  eval_steps: 10
  save_total_limit: 1
  gradient_accumulation_steps: 1
  dataloader_num_workers: 4
  min_learning_rate: 1e-5
  from_pretrained: /mnt/ssd2/hazem/Projects/CST/MelCausalVAE/checkpoints/MelCausalVAE/exp-efficient/checkpoint-8910/model.safetensors
  #resume_from_checkpoint: /workspace/MelCausalVAE/checkpoints/MelCausalVAE/exp-efficient/checkpoint-3000
  phonemes: true

convformer:
  compress_factor_C: 2
  tf_heads: 16
  tf_layers: 8
  latent_dim: 64
  n_residual_blocks: 3
  logvar_layer: false
  use_sofplus: false
  target_std: 1.0
  kl_loss_warmup_steps: 250
  kl_loss_weight: 1e-3

cfm:
  unet_dim: 512
  unet_depth: 8
  unet_heads: 16
  unet_dropout_rate: 0.1
  use_conv_layer: true
  audio_latent_dim: 64
  uncond_prob: 0.15
  learned_prior: false
  is_causal: true