training:
  dataset_name: librispeech100h
  output_dir: checkpoints/MelCausalVAE/variable-rate # variable rate training
  num_train_epochs: 2
  learning_rate: 1e-4
  per_device_train_batch_size: 2
  report_to: none
  wandb_project: convformer-cfm
  wandb_run_name: variable-rate-training-librispeech100h
  fp16: false
  bf16: true
  save_steps: 1000
  eval_strategy: "no"
  eval_steps: 1000
  save_total_limit: 1
  gradient_accumulation_steps: 10
  dataloader_num_workers: 4
  #from_pretrained: /home/ec2-user/checkpoints/MelCausalVAE/setting3/checkpoint-16000/model.safetensors
  #resume_from_checkpoint: checkpoints/MelCausalVAE/setting5/checkpoint-1000

convformer:
  compress_factor_C: 2
  tf_heads: 4
  tf_layers: 4
  latent_dim: 64
  n_residual_blocks: 1
  logvar_layer: true
  use_sofplus: false
  target_std: 1.0
  kl_loss_warmup_steps: 100
  kl_loss_weight: 1e-2

cfm:
  unet_dim: 256
  unet_depth: 4
  unet_heads: 8
  unet_dropout_rate: 0.1
  use_conv_layer: true
  audio_latent_dim: 64
  uncond_prob: 0.15
  learned_prior: false
  is_causal: false